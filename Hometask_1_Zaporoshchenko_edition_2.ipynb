{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrdL__lCtr6M"
   },
   "source": [
    "# Анна Запорощенко"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_CzjN-StydR"
   },
   "source": [
    "В рамках этого задания мы будем создавать программу, которая получая на вход отзыв, будет предсказывать, является ли отзыв положительным или отрицательным. Делать мы будем это таким образом: мы возьмём некоторое число заранее размеченных как положительные или отрицательные отзывов, выделим те слова, которые встречаются только в положительных или только в отрицательных отзывах, и будем считать, каких слов  в поступившем нам на проверку отзыве больше.\n",
    "\n",
    "\n",
    "Мы будем работать по заранее определённому пайплайну:\n",
    "\n",
    "1.  Сначала нам надо скачать дату -- соберите как минимум 60 (30 положительных  и 30 отрицательных) отзывов на похожие продукты (не надо мешать отзывы на отели с отзывами на ноутбуки) для составления \" тонального словаря\" (чем больше отзывов, тем лучше)  и 10 отзывов для проверки качества.   3 балла в случае сбора путём парсинга, 1 - если найдете уже готовые данные или просто закопипастите без парсинга\n",
    "\n",
    "2. Токенизируйте слова,  приведите их к нижнему регистру и к начальной форме  (1 балл за токенизацию, 1 - за начальную форму)\n",
    "\n",
    "3. Составьте 2 множества - в одном будут слова, которые встречаются только в положительных отзывах, а в другом - встречающиеся только в отрицательных. Попробуйте поиграть с частотностями и исключить шум (к примеру, выбросить слова, встречающиеся 1-2 раза) (3 балла) (если у вас получились пустые множества, уберите фильтр по частотности или увеличьте выборку)\n",
    "\n",
    "4. Создайте функцию, которая будет определять, положительный ли отзыв или отрицательный в зависимости от того, какие слова встретились в нём, и посчитайте качество при помощи accuracy (1  - за коректно работающую функцию, 1 - за подсчёт accuracy)\n",
    "\n",
    "5. Предложите как минимум 2 способа улучшить эту программу с помощью добавления к ней любых мулек  - просто словами, писать улучшающий код не надо (1 балл)\n",
    "\n",
    "6. Логичность и чистота кода (1 балл) \n",
    "\n",
    "Да, тут 12 баллов - два можно потерять.\n",
    "\n",
    "\n",
    "В случае, если после долгих мучений в п. 3 множества по объективным причинам не получается (покажите, что пытались) - отправляйте жабу - зачьтём полный балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "sMBivptux1xV",
    "outputId": "18135eb5-0550-49bb-bd26-865dfc2c744c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VADIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VADIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "igHuEg7R7Tyi"
   },
   "outputs": [],
   "source": [
    "#Функция очищает список текстов от тегов и ненужных знаков\n",
    "def parse(info: list) -> list:\n",
    "    reviews = []\n",
    "    for i in info:\n",
    "        n = re.sub('<[A-Za-z\\/][^>]*>|\\r', '', i[1])\n",
    "        reviews.append(re.sub('\\n', '. ', n))\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "go-NP2ubgJek"
   },
   "outputs": [],
   "source": [
    "#Функция принимает ссылку и ищет сначала все отзывы с самой низкой оценкой, затем - с самой высокой. \n",
    "#Возвращает по 44 отзыва каждого вида общим списком.\n",
    "def get_reviews(url: str) -> list:\n",
    "    user_agent = UserAgent().chrome\n",
    "    req = urllib.request.Request(url, headers={'User-Agent':user_agent})\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        html_content = response.read().decode('utf-8')\n",
    "    bad_reviews = parse(re.findall('<b>1</b></span> из 10(.+?)jrReviewComment\"><div >(.+?)</div>', html_content, re.DOTALL))\n",
    "    good_reviews = parse(re.findall('<b>10</b></span> из 10(.+?)jrReviewComment\"><div >(.+?)</div>', html_content, re.DOTALL))\n",
    "    return bad_reviews[0:44] + good_reviews[0:44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция принимает список отзывов. Каждый отзыв заменяет на список лемм, которые точно состоят \n",
    "#из букв и не являются стоп-словами. Все слова приводятся к нижнему регистру. Функция возвращает матрицу\n",
    "#со списками лемм для каждого отзыва.\n",
    "#Вряд ли сработает на Google Colab, потому что Mystem.lemmatize() подвисает на нем\n",
    "def preprocessing(reviews: list) -> list:\n",
    "    russian_stopwords = stopwords.words('russian')\n",
    "    m = Mystem()\n",
    "    reviews_new = []\n",
    "    for review in tqdm(reviews):\n",
    "        review_new = []\n",
    "        for word in m.lemmatize(review):\n",
    "            if (word.isalpha())and(word not in russian_stopwords):\n",
    "                review_new.append(word.lower())\n",
    "        reviews_new.append(review_new)\n",
    "    return reviews_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!NEW CODE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_2(reviews: list, token_reviews: list) -> list:\n",
    "    m = Mystem()\n",
    "    for r in tqdm(enumerate(reviews)):\n",
    "        doc = Doc(r[1])\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.parse_syntax(syntax_parser)\n",
    "        heads = []\n",
    "        for token in doc.tokens:\n",
    "            if (token.pos == 'NOUN')or(token.pos == 'ADJ')or(token.pos == 'PREP'):\n",
    "                heads.append(token.id)\n",
    "        for head in heads:\n",
    "            collacation = ''\n",
    "            for token in doc.tokens:\n",
    "                if ((token.id == head)or(token.head_id == head))and(token.text.isalpha() == True):\n",
    "                    collacation += m.lemmatize(token.text.lower())[0] + ' '\n",
    "            if len(collacation.strip(' ').split(' ')) > 1:\n",
    "                token_reviews[r[0]].append(collacation.strip(' '))\n",
    "    return token_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\urllib\\request.py\", line 1350, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\http\\client.py\", line 1240, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\http\\client.py\", line 1286, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\http\\client.py\", line 1235, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\http\\client.py\", line 1006, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\http\\client.py\", line 946, in send\n",
      "    self.connect()\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\http\\client.py\", line 917, in connect\n",
      "    self.sock = self._create_connection(\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\socket.py\", line 808, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\socket.py\", line 796, in create_connection\n",
      "    sock.connect(sa)\n",
      "socket.timeout: timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\site-packages\\fake_useragent\\utils.py\", line 64, in get\n",
      "    with contextlib.closing(urlopen(\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\urllib\\request.py\", line 222, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\urllib\\request.py\", line 525, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\urllib\\request.py\", line 542, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\urllib\\request.py\", line 502, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\urllib\\request.py\", line 1379, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\urllib\\request.py\", line 1353, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error timed out>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\site-packages\\fake_useragent\\utils.py\", line 164, in load\n",
      "    browsers_dict[browser_key] = get_browser_versions(\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\site-packages\\fake_useragent\\utils.py\", line 120, in get_browser_versions\n",
      "    html = get(\n",
      "  File \"C:\\Users\\VADIK\\anaconda3\\lib\\site-packages\\fake_useragent\\utils.py\", line 84, in get\n",
      "    raise FakeUserAgentError('Maximum amount of retries reached')\n",
      "fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached\n"
     ]
    }
   ],
   "source": [
    "reviews = get_reviews('https://www.megacritic.ru/film/prometej')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 88/88 [01:42<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "token_reviews = preprocessing(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [2:05:01, 85.24s/it] \n"
     ]
    }
   ],
   "source": [
    "collacations_and_words = preprocessing_2(reviews, token_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!NEW CODE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "YmEW9qwWNbU3",
    "outputId": "1deebe5b-1baa-4213-e95a-b2e9dbc15e9e"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(collacations_and_words),\n",
    "                                                    np.concatenate((np.zeros(44), np.ones(44))),\n",
    "                                                    test_size=10/88,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Списки лемм из отрицательных и положительных отзывов X_train помещаются в два отдельных списка, \n",
    "#из них потом создаются частотные словари.Также создается частотный словарь по всем отзывам X_train.\n",
    "bad_reviews = []\n",
    "good_reviews = []\n",
    "for y in enumerate(y_train):\n",
    "    if y[1] == 0:\n",
    "        bad_reviews.extend(X_train[y[0]])\n",
    "    else:\n",
    "        good_reviews.extend(X_train[y[0]])\n",
    "bad_reviews = Counter(bad_reviews)\n",
    "good_reviews = Counter(good_reviews)\n",
    "reviews = Counter(sum(X_train.tolist(), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция принимает частотные словари отрицательных и положительных отзывов, а также общий частотный словарь.\n",
    "#Она создает словарь со словами, которые присутствуют и в положительных, и в отрицательных отзывах.\n",
    "def check (bad_reviews: Counter, good_reviews: Counter, reviews: Counter) -> Counter:\n",
    "    words = Counter()\n",
    "    for word, value in reviews.items():\n",
    "        if (word in bad_reviews.keys())and(word in good_reviews.keys()):\n",
    "            words[word] = value\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция позволяет регулировать, насколько часто встречающиеся слова отправлять в функцию check.\n",
    "def cutter (reviews: Counter, start: int, stop: int) -> Counter:\n",
    "    new_reviews = Counter()\n",
    "    for word, value in reviews.items():\n",
    "        if (value >= start)and(value <= stop):\n",
    "            new_reviews[word] = value\n",
    "    return new_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно проверить, есть ли какая-то частотность, при которой конкретная лемма встречается только в одном виде отзывов. Единица не дает нам какой-либо информации, с частотностью 19 на самом деле есть только 1 слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "частотность слова  1  количество повторов  0\n",
      "частотность слова  2  количество повторов  148\n",
      "частотность слова  3  количество повторов  82\n",
      "частотность слова  4  количество повторов  59\n",
      "частотность слова  5  количество повторов  30\n",
      "частотность слова  6  количество повторов  26\n",
      "частотность слова  7  количество повторов  16\n",
      "частотность слова  8  количество повторов  11\n",
      "частотность слова  9  количество повторов  6\n",
      "частотность слова  10  количество повторов  5\n",
      "частотность слова  11  количество повторов  11\n",
      "частотность слова  12  количество повторов  4\n",
      "частотность слова  13  количество повторов  3\n",
      "частотность слова  14  количество повторов  3\n",
      "частотность слова  15  количество повторов  2\n",
      "частотность слова  17  количество повторов  5\n",
      "частотность слова  18  количество повторов  1\n",
      "частотность слова  19  количество повторов  0\n",
      "частотность слова  20  количество повторов  2\n",
      "частотность слова  22  количество повторов  2\n",
      "частотность слова  23  количество повторов  2\n",
      "частотность слова  24  количество повторов  1\n",
      "частотность слова  27  количество повторов  3\n",
      "частотность слова  28  количество повторов  1\n",
      "частотность слова  34  количество повторов  1\n",
      "частотность слова  40  количество повторов  1\n",
      "частотность слова  43  количество повторов  1\n",
      "частотность слова  65  количество повторов  1\n",
      "частотность слова  152  количество повторов  1\n"
     ]
    }
   ],
   "source": [
    "numbers = list(Counter(reviews.values()).keys())\n",
    "numbers.sort()\n",
    "for number in numbers:\n",
    "    print('частотность слова ', number, ' количество повторов ', \n",
    "          len(check(bad_reviews, good_reviews, cutter(reviews, number, number))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зато можно увидеть, что слова перестают создавать больше количество \"шума\", когда встречаются более 6 раз. Ровно разделить все слова на \"положительные\" и \"отрицательные\" не получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral = check(bad_reviews, good_reviews, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(neutral: Counter, polar: Counter) -> Counter:\n",
    "    new = Counter()\n",
    "    for word, value in polar.items():\n",
    "        if word not in neutral: #убираем все слова с частотностью меньше 3\n",
    "            new[word] = value\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_reviews = get_words(neutral, bad_reviews)\n",
    "good_reviews = get_words(neutral, good_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция проверяет каждое слово в отдельном отзыве.\n",
    "#Если слово есть в словаре негативных отзывов, score получает -1, если в словаре положительных - +1.\n",
    "#Таким образом, каждый отзыв насчитывает свой score, в зависимости от которого происходит предсказание.\n",
    "def predict(X_test: np.array, bad_reviews: Counter, good_reviews: Counter) -> np.array:\n",
    "    results = []\n",
    "    for review in X_test:\n",
    "        score = 0\n",
    "        for word in review:\n",
    "            if word in bad_reviews.keys():\n",
    "                score -= 1\n",
    "            elif word in good_reviews.keys():\n",
    "                score +=1\n",
    "            else:\n",
    "                pass\n",
    "        if score < 0:\n",
    "            results.append(0)\n",
    "        else:\n",
    "            results.append(1)\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predict(X_test, bad_reviews, good_reviews), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ничего не изменилось 😭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предложения\n",
    "1) Учитывать n-граммы.\n",
    "2) \"Нейтральные\" слова тоже могли бы быть полезными: если слово встретилось 1 раз в положительных отзывах и 40 в отрицательных, исключать его при этом объеме данных невыгодно."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hometask 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
