{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrdL__lCtr6M"
   },
   "source": [
    "# Анна Запорощенко"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_CzjN-StydR"
   },
   "source": [
    "В рамках этого задания мы будем создавать программу, которая получая на вход отзыв, будет предсказывать, является ли отзыв положительным или отрицательным. Делать мы будем это таким образом: мы возьмём некоторое число заранее размеченных как положительные или отрицательные отзывов, выделим те слова, которые встречаются только в положительных или только в отрицательных отзывах, и будем считать, каких слов  в поступившем нам на проверку отзыве больше.\n",
    "\n",
    "\n",
    "Мы будем работать по заранее определённому пайплайну:\n",
    "\n",
    "1.  Сначала нам надо скачать дату -- соберите как минимум 60 (30 положительных  и 30 отрицательных) отзывов на похожие продукты (не надо мешать отзывы на отели с отзывами на ноутбуки) для составления \" тонального словаря\" (чем больше отзывов, тем лучше)  и 10 отзывов для проверки качества.   3 балла в случае сбора путём парсинга, 1 - если найдете уже готовые данные или просто закопипастите без парсинга\n",
    "\n",
    "2. Токенизируйте слова,  приведите их к нижнему регистру и к начальной форме  (1 балл за токенизацию, 1 - за начальную форму)\n",
    "\n",
    "3. Составьте 2 множества - в одном будут слова, которые встречаются только в положительных отзывах, а в другом - встречающиеся только в отрицательных. Попробуйте поиграть с частотностями и исключить шум (к примеру, выбросить слова, встречающиеся 1-2 раза) (3 балла) (если у вас получились пустые множества, уберите фильтр по частотности или увеличьте выборку)\n",
    "\n",
    "4. Создайте функцию, которая будет определять, положительный ли отзыв или отрицательный в зависимости от того, какие слова встретились в нём, и посчитайте качество при помощи accuracy (1  - за коректно работающую функцию, 1 - за подсчёт accuracy)\n",
    "\n",
    "5. Предложите как минимум 2 способа улучшить эту программу с помощью добавления к ней любых мулек  - просто словами, писать улучшающий код не надо (1 балл)\n",
    "\n",
    "6. Логичность и чистота кода (1 балл) \n",
    "\n",
    "Да, тут 12 баллов - два можно потерять.\n",
    "\n",
    "\n",
    "В случае, если после долгих мучений в п. 3 множества по объективным причинам не получается (покажите, что пытались) - отправляйте жабу - зачьтём полный балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "sMBivptux1xV",
    "outputId": "18135eb5-0550-49bb-bd26-865dfc2c744c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VADIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VADIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "igHuEg7R7Tyi"
   },
   "outputs": [],
   "source": [
    "#Функция очищает список текстов от тегов и ненужных знаков\n",
    "def parse(info: list) -> list:\n",
    "    reviews = []\n",
    "    for i in info:\n",
    "        n = re.sub('<[A-Za-z\\/][^>]*>|\\r', '', i[1])\n",
    "        reviews.append(re.sub('\\n', '. ', n))\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "go-NP2ubgJek"
   },
   "outputs": [],
   "source": [
    "#Функция принимает ссылку и ищет сначала все отзывы с самой низкой оценкой, затем - с самой высокой. \n",
    "#Возвращает по 44 отзыва каждого вида общим списком.\n",
    "def get_reviews(url: str) -> list:\n",
    "    user_agent = UserAgent().chrome\n",
    "    req = urllib.request.Request(url, headers={'User-Agent':user_agent})\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        html_content = response.read().decode('utf-8')\n",
    "    bad_reviews = parse(re.findall('<b>1</b></span> из 10(.+?)jrReviewComment\"><div >(.+?)</div>', html_content, re.DOTALL))\n",
    "    good_reviews = parse(re.findall('<b>10</b></span> из 10(.+?)jrReviewComment\"><div >(.+?)</div>', html_content, re.DOTALL))\n",
    "    return bad_reviews[0:44] + good_reviews[0:44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция принимает список отзывов. Каждый отзыв заменяет на список лемм, которые точно состоят \n",
    "#из букв и не являются стоп-словами. Все слова приводятся к нижнему регистру. Функция возвращает матрицу\n",
    "#со списками лемм для каждого отзыва.\n",
    "#Вряд ли сработает на Google Colab, потому что Mystem.lemmatize() подвисает на нем\n",
    "def preprocessing(reviews: list) -> np.array:\n",
    "    russian_stopwords = stopwords.words('russian')\n",
    "    m = Mystem()\n",
    "    for review in tqdm(enumerate(reviews)):\n",
    "        reviews[review[0]] = []\n",
    "        for word in m.lemmatize(review[1]):\n",
    "            if (word.isalpha())and(word not in russian_stopwords):\n",
    "                reviews[review[0]].append(word.lower())\n",
    "    return np.array(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "YmEW9qwWNbU3",
    "outputId": "1deebe5b-1baa-4213-e95a-b2e9dbc15e9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [01:42,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(preprocessing(get_reviews('https://www.megacritic.ru/film/prometej')),\n",
    "                                                    np.concatenate((np.zeros(44), np.ones(44))),\n",
    "                                                    test_size=10/88,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Списки лемм из отрицательных и положительных отзывов X_train помещаются в два отдельных списка, \n",
    "#из них потом создаются частотные словари.Также создается частотный словарь по всем отзывам X_train.\n",
    "bad_reviews = []\n",
    "good_reviews = []\n",
    "for y in enumerate(y_train):\n",
    "    if y[1] == 0:\n",
    "        bad_reviews.extend(X_train[y[0]])\n",
    "    else:\n",
    "        good_reviews.extend(X_train[y[0]])\n",
    "bad_reviews = Counter(bad_reviews)\n",
    "good_reviews = Counter(good_reviews)\n",
    "reviews = Counter(sum(X_train.tolist(), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция принимает частотные словари отрицательных и положительных отзывов, а также общий частотный словарь.\n",
    "#Она создает словарь со словами, которые присутствуют и в положительных, и в отрицательных отзывах.\n",
    "def check (bad_reviews: Counter, good_reviews: Counter, reviews: Counter) -> Counter:\n",
    "    words = Counter()\n",
    "    for word, value in reviews.items():\n",
    "        if (word in bad_reviews.keys())and(word in good_reviews.keys()):\n",
    "            words[word] = value\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция позволяет регулировать, насколько часто встречающиеся слова отправлять в функцию check.\n",
    "def cutter (reviews: Counter, start: int, stop: int) -> Counter:\n",
    "    new_reviews = Counter()\n",
    "    for word, value in reviews.items():\n",
    "        if (value >= start)and(value <= stop):\n",
    "            new_reviews[word] = value\n",
    "    return new_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно проверить, есть ли какая-то частотность, при которой конкретная лемма встречается только в одном виде отзывов. Единица не дает нам какой-либо информации, с частотностью 19 на самом деле есть только 1 слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "частотность слова  1  количество повторов  0\n",
      "частотность слова  2  количество повторов  137\n",
      "частотность слова  3  количество повторов  75\n",
      "частотность слова  4  количество повторов  56\n",
      "частотность слова  5  количество повторов  30\n",
      "частотность слова  6  количество повторов  25\n",
      "частотность слова  7  количество повторов  16\n",
      "частотность слова  8  количество повторов  10\n",
      "частотность слова  9  количество повторов  6\n",
      "частотность слова  10  количество повторов  5\n",
      "частотность слова  11  количество повторов  11\n",
      "частотность слова  12  количество повторов  3\n",
      "частотность слова  13  количество повторов  3\n",
      "частотность слова  14  количество повторов  3\n",
      "частотность слова  15  количество повторов  2\n",
      "частотность слова  17  количество повторов  5\n",
      "частотность слова  18  количество повторов  1\n",
      "частотность слова  19  количество повторов  0\n",
      "частотность слова  20  количество повторов  2\n",
      "частотность слова  22  количество повторов  2\n",
      "частотность слова  23  количество повторов  2\n",
      "частотность слова  24  количество повторов  1\n",
      "частотность слова  27  количество повторов  3\n",
      "частотность слова  28  количество повторов  1\n",
      "частотность слова  34  количество повторов  1\n",
      "частотность слова  40  количество повторов  1\n",
      "частотность слова  43  количество повторов  1\n",
      "частотность слова  65  количество повторов  1\n",
      "частотность слова  152  количество повторов  1\n"
     ]
    }
   ],
   "source": [
    "numbers = list(Counter(reviews.values()).keys())\n",
    "numbers.sort()\n",
    "for number in numbers:\n",
    "    print('частотность слова ', number, ' количество повторов ', \n",
    "          len(check(bad_reviews, good_reviews, cutter(reviews, number, number))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зато можно увидеть, что слова перестают создавать больше количество \"шума\", когда встречаются более 6 раз. Ровно разделить все слова на \"положительные\" и \"отрицательные\" не получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral = check(bad_reviews, good_reviews, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(neutral: Counter, polar: Counter) -> Counter:\n",
    "    new = Counter()\n",
    "    for word, value in polar.items():\n",
    "        if word not in neutral:\n",
    "            new[word] = value\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_reviews = get_words(neutral, bad_reviews)\n",
    "good_reviews = get_words(neutral, good_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы преобразовать тексты в вектора, будем исользовать метод bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция склеивает леммы внутри каждого отзыва через пробел, чтобы тексты можно было передать в CountVectoriser()\n",
    "def joined(massive: np.array) -> np.array:\n",
    "    for review in enumerate(massive):\n",
    "        massive[review[0]] = (' ').join(review[1])\n",
    "    return massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = joined(X_train)\n",
    "X_test = joined(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для общего словаря использую только X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<78x1985 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3726 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(lr_model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предложения\n",
    "1) Можно убрать \"шум\" (нечастотные слова из общего словаря), и тогда, возможно, результат будет лучше.\n",
    "2) Использовать не просто частотность, а tf idf было бы выгоднее: слова, которые одинаково часто встречаются и в положительных, и в отрицательных отзывах, но которые не были исключены из словаря как стоп-слова (та самая нейтральная лексика), имели бы маленький вес в векторах."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hometask 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
