{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анна Запорощенко"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На последнем семинаре мы проанализировали несколько различных морфологических теггеров. Как же узнать, какой использовать? Давайте сравним их качество!\n",
    "В этой домашке вам будет нужно найти или самим написать русский и английский тексты (каждый от ста слов), в которых  будут какие-то трудные или неоднозначные для POS теггинга моменты и разметить их в ручную (а потом объяснить, какие моменты вы тут считаете трудными для автоматического посттеггинга и почему) – с помощью этих текстов мы будем оценивать качество работы наших теггоров. В текстах размечаем только части речи, ничего больше!\n",
    "Вы получаете балл за создание, разметку текста и объяснение того, почему этот текст подходит для оценки (что в нём сложного). Всего за этот пункт 2 балла, т. к. языка 2.\n",
    "Потом вам будет нужно взять три  POS теггера для русского (pymorphy2, mysteam, Natasha) и 3  - для английского (SpyCy, Flair, NLTK) и «прогнать» текст через каждый из них (если вы запустите только 2 теггера из трёх – получите балл, если три  из трёх – 2 балла, т. е. суммарно за этот пункт можно получить 4 балла).\n",
    "После этого вам надо будет оценить accuracy для каждого теггера. Заметьте, что в разных системах имена теггов и части речи  могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции или кода и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции. Этот пункт стоит 2 балла.\n",
    "Тут вы уже получили 8 баллов.\n",
    "Дальше вам нужно взять лучший теггер для русского языка и  с его помощью написать функцию, которая повысит качество работы программы из первой домашки. Так, многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Вам надо выделить 3 вида синтаксических групп (к примеру не + какая-то часть речи или NP или сущ.+ наречие или еще что-то), запись которых в словарь, по вашему мнению, улучшила бы качество работы программы и создать такую функцию или функции, которые с помощью любых известных нам средств (chunking и regexp grammar, Natasha syntax parser,  код с последнего занятия по SpyCy, etc.) будет выделять эти группы в поданном в нее тексте. Два балла за саму функцию, балл за объяснение того, почему именно эти группы вы взяли.\n",
    "2 бонусных балла, если встроите эту функцию в программу из предыдущей домашки и сравните качества работы программы с нею и без неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\VADIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\VADIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "import spacy\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    text = word_tokenize(text)\n",
    "    text_clean =[]\n",
    "    for word in text:\n",
    "        if (word not in '.,/\\|!?:;*—- '):\n",
    "            text_clean.append(word)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rus = 'Германия, Дания и Нидерланды намерены заключить соглашение о строительстве в Северном море искусственного острова, \\\n",
    "на котором будет производиться электроэнергия для снабжения электричеством 80 миллионов человек. По проекту на острове под \\\n",
    "названием Power Link площадью около 6 км² оборудуют порт, взлетно-посадочную полосу, жилые и технические сооружения. \\\n",
    "Но самое главное — остров будет окружен тысячами ветровых турбин, предназначенных для производства экологически чистой \\\n",
    "энергии. Остров построят на Доггер-банке — большой песчаной отмели в Северном море, примерно на равном расстоянии от \\\n",
    "Великобритании, Скандинавии, Германии и стран Бенилюкса. Он будет соединен с этими территориями подводными кабелями. \\\n",
    "Глубина моря в районе отмели колеблется от 15 до 36 метров, что позволит без особых сложностей установить 10 тысяч ветровых \\\n",
    "турбин. Кроме того, на Доггер-банке всегда очень ветрено — этот факт также повлиял на выбор места для возведения острова. \\\n",
    "Остров станет своего рода распределительным центром, через который энергия от ветровых электростанций будет подаваться в \\\n",
    "страны ЕС: в Нидерланды, Данию, Германию и др. Автор статьи: Мария Смирных.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rus = ['NOUN', 'NOUN', 'CONJ', 'NOUN', 'ADJ', 'VERB', 'NOUN', 'PREP', 'NOUN', 'PREP', 'ADJ', \n",
    "       'NOUN', 'ADJ', 'NOUN', 'PREP', 'PRO', 'VERB', 'VERB', 'NOUN', 'PRO', 'NOUN', 'NOUN',\n",
    "       'NUMR', 'NUMR', 'NOUN',\n",
    "       'PREP', 'NOUN', 'PREP', 'NOUN', 'PREP', 'NOUN', 'OTHER', 'OTHER', 'NOUN', 'PREP', 'NUMR',\n",
    "        'NOUN', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'CONJ', 'ADJ', 'NOUN',\n",
    "       'CONJ', 'PRO', 'NOUN', 'NOUN', 'VERB', 'VERB', 'NUMR', 'ADJ', 'NOUN', 'VERB', 'PREP', 'NOUN', \n",
    "        'ADVB', 'ADJ', 'NOUN',\n",
    "       'NOUN', 'VERB', 'PREP', 'NOUN', 'ADJ', 'ADJ', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'ADVB', 'PREP',\n",
    "      'ADJ', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'NOUN',\n",
    "       'PRO', 'VERB', 'VERB', 'PREP', 'PRO', 'NOUN', 'ADJ', 'NOUN',\n",
    "       'NOUN', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'VERB', 'PREP', 'NUMR', 'PREP', 'NUMR', 'NOUN', 'CONJ', 'VERB', \n",
    "       'PREP', 'ADJ', 'NOUN', 'VERB', 'NUMR', 'NUMR', 'ADJ', 'NOUN',\n",
    "       'PREP', 'PRO', 'PREP', 'NOUN', 'ADVB', 'ADVB', 'ADVB', 'PRO', 'NOUN', 'CONJ', 'VERB', 'PREP',\n",
    "      'NOUN', 'NOUN', 'PREP', 'NOUN', 'NOUN',\n",
    "       'NOUN', 'VERB', 'PRO', 'NOUN', 'ADJ', 'NOUN', 'PREP', 'PRO', 'NOUN', 'PREP', 'ADJ', 'NOUN', \n",
    "       'VERB', 'VERB', 'PREP', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'CONJ', 'PRO',\n",
    "       'NOUN', 'NOUN', 'NOUN', 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемы для русского языка:\n",
    "* Слова с дефисом, неразделимые: Доггер-банк\n",
    "* Фамилии, похожие на прилагательные: Смирных\n",
    "* Аббревиатуры: ЕС\n",
    "* Сокращения: др., км²\n",
    "* Частеричные категории: мест./прил.(который, самый), сущ./числ. (миллионов), предл./нар. (около), сущ./прил. (главное), прил./гл. (предназначенных)\n",
    "* Омонимия: сущ./гл. (отмели)\n",
    "* Редкие слова: Бинелюкса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_rus = cleaner(text_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rus_pymorphy = []\n",
    "morph0 = MorphAnalyzer()\n",
    "for p in tokens_rus:\n",
    "    p = morph0.parse(p)[0]\n",
    "    p = str(p.tag.POS)\n",
    "    if (p == 'PRTS')or(p == 'INFN')or(p == 'PRTF')or(p == 'GRND'):\n",
    "        p = 'VERB'\n",
    "    if (p == 'ADJF')or(p == 'ADJS'):\n",
    "        p = 'ADJ'\n",
    "    if p == 'None':\n",
    "        p = 'OTHER'\n",
    "    if p == 'NPRO':\n",
    "        p = 'PRO'\n",
    "    pos_rus_pymorphy.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'NOUN',\n",
       " 'CONJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'ADJ',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'OTHER',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'OTHER',\n",
       " 'OTHER',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'OTHER',\n",
       " 'OTHER',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'CONJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'CONJ',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'ADVB',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'ADVB',\n",
       " 'PREP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'CONJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PRO',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'PREP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PREP',\n",
       " 'OTHER',\n",
       " 'PREP',\n",
       " 'OTHER',\n",
       " 'NOUN',\n",
       " 'CONJ',\n",
       " 'VERB',\n",
       " 'PREP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'OTHER',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'ADJ',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'ADVB',\n",
       " 'ADVB',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PRCL',\n",
       " 'VERB',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PREP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'CONJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADJ']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_rus_pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441558441558441"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pos_rus_pymorphy, pos_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph1 = Mystem()\n",
    "pos_rus_mystem = morph1.analyze((' ').join(tokens_rus))[:-1]\n",
    "val = {'text': ' '}\n",
    "while val in pos_rus_mystem:\n",
    "    pos_rus_mystem.remove(val)\n",
    "for pos in enumerate(pos_rus_mystem):\n",
    "    if ('analysis' in pos[1])and(len(pos[1]['analysis']) != 0):\n",
    "        pos_rus_mystem[pos[0]] = re.split(',|=', pos[1]['analysis'][0]['gr'])[0]\n",
    "    elif pos[1]['text'] == '-':\n",
    "        pos_rus_mystem[pos[0]] = 'PUNKT'\n",
    "    else:\n",
    "        pos_rus_mystem[pos[0]] = 'OTHER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos in enumerate(pos_rus_mystem):\n",
    "    if pos[1] == 'S':\n",
    "        pos_rus_mystem[pos[0]] = 'NOUN'\n",
    "    if pos[1] == 'A':\n",
    "        pos_rus_mystem[pos[0]] = 'ADJ'\n",
    "    if pos[1] == 'PR':\n",
    "        pos_rus_mystem[pos[0]] = 'PREP'\n",
    "    if (pos[1] == 'APRO')or(pos[1] == 'ADVPRO')or(pos[1] == 'SPRO'):\n",
    "        pos_rus_mystem[pos[0]] = 'PRO'\n",
    "    if pos[1] == 'V':\n",
    "        pos_rus_mystem[pos[0]] = 'VERB'\n",
    "    if pos[1] == 'ADV':\n",
    "        pos_rus_mystem[pos[0]] = 'ADVB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rus_mystem = re.sub(' [A-Z]+? PUNKT [A-Z]+? ', ' FALSE ', ' '.join(pos_rus_mystem)).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_rus_mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8961038961038961"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pos_rus_mystem, pos_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc((' ').join(tokens_rus))\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "pos_rus_natasha = []\n",
    "for token in doc.tokens:\n",
    "    pos = re.findall(\"pos='(.+?)'\",str(token))[0]\n",
    "    if pos == 'PROPN':\n",
    "        pos = 'NOUN'\n",
    "    if (pos == 'PRON')or(pos == 'DET'):\n",
    "        pos = 'PRO'\n",
    "    if pos == 'AUX':\n",
    "        pos = 'VERB'\n",
    "    if pos == 'ADP':\n",
    "        pos = 'PREP'\n",
    "    if pos == 'CCONJ':\n",
    "        pos = 'CONJ'\n",
    "    if pos == 'X':\n",
    "        pos = 'OTHER'\n",
    "    if pos == 'NUM':\n",
    "        pos = 'NUMR'\n",
    "    if pos == 'ADV':\n",
    "        pos = 'ADVB'\n",
    "    pos_rus_natasha.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_rus_natasha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935064935064935"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pos_rus_natasha, pos_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = \"This condition is in contradictory to the large θc as is expected for sessile droplets on highly \\\n",
    "lyophobic surfaces, which makes it difficult to produce uniform OSC films by conventional solution processes. \\\n",
    "So, it's important and necessary to control the shape of solution meniscus on the lyophobic surfaces. Here, \\\n",
    "we report the development of an extended meniscus coating technique that allows to manufacture \\\n",
    "OSC single crystal films on top of highly lyophobic Cytop gate dielectric layer. The uniform film growth becomes \\\n",
    "possible by keeping extension of solution meniscus on the Cytop surface. The electromagnetic coating allowed us \\\n",
    "to integrate single crystal films of small molecular OSCs and Cytop gate dielectric in BGBC type \\\n",
    "organic TFTs, resulting in an extremely small SS value of 63 mV dec1, which is close to the theoretical limit.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_en = ['DET', 'NOUN', 'VERB', 'PREP', 'NOUN', 'PREP', 'DET', 'ADJ', 'OTHER', 'CONJ', 'VERB', 'VERB', 'PREP',\n",
    "          'ADJ', 'NOUN', 'PREP', 'ADVB', 'ADJ', 'NOUN', 'PRO', 'VERB', 'PRO', 'ADJ', 'PREP', 'VERB', 'ADJ', 'NOUN',\n",
    "          'NOUN', 'PREP', 'ADJ', 'NOUN', 'NOUN',\n",
    "         'CONJ', 'PRO', 'VERB', 'ADJ', 'CONJ', 'ADJ', 'PREP', 'VERB', 'DET', 'NOUN', 'PREP', 'NOUN', 'NOUN', \n",
    "         'PREP', 'DET', 'ADJ', 'NOUN',\n",
    "         'ADVB', 'PRO', 'VERB', 'DET', 'NOUN', 'PREP', 'DET', 'VERB', 'NOUN', 'VERB', 'NOUN', 'DET',\n",
    "         'VERB', 'PREP', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'PREP', 'ADVB', 'ADJ', 'NOUN', \n",
    "         'NOUN', 'NOUN', 'NOUN',\n",
    "         'DET', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'PREP', 'VERB', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'PREP', 'DET',\n",
    "         'NOUN', 'NOUN',\n",
    "         'DET', 'ADJ', 'NOUN', 'VERB', 'PRO', 'PREP', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'PREP', 'ADJ', 'ADJ',\n",
    "         'NOUN', 'CONJ', 'NOUN', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'PREP', 'DET', \n",
    "         'ADVB', 'ADJ', 'NOUN', 'NOUN', 'PREP', 'NUMR', 'OTHER', 'OTHER', 'PRO', 'VERB', 'ADJ', 'PREP', 'DET', 'ADJ',\n",
    "         'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_en = cleaner(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемы для английского языка:\n",
    "* Сокращения: it's\n",
    "* Омонимия: as (conj/prep), uniform (adj/noun), so (advb/conj), manufacture(verb/noun)\n",
    "* Герундий\n",
    "* Аббревиатура: OSC, SS\n",
    "* Специализированные слова, в т. ч. имена собственные: sessile, meniscus, Cytop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nltk.pos_tag(tokens_en)\n",
    "pos_en_nltk = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos in pos_en:\n",
    "    if pos[1] == 'DT':\n",
    "        p = 'DET'\n",
    "    if (pos[1] == 'NN')or(pos[1] == 'NNP')or(pos[1] == 'NNS'):\n",
    "        p = 'NOUN'\n",
    "    if (pos[1] == 'VBZ')or(pos[1] == 'VB'):\n",
    "        p = 'VERB'\n",
    "    if (pos[1] == 'TO')or(pos[1] == 'IN'):\n",
    "        p = 'PREP'\n",
    "    if pos[1] == 'JJ':\n",
    "        p = 'ADJ'\n",
    "    if pos[1] == 'CC':\n",
    "        p = 'CONJ'\n",
    "    if pos[1] == 'RB':\n",
    "        p = 'ADVB'\n",
    "    if (pos[1] == 'PRP')or(pos[1] == 'WDT'):\n",
    "        p = 'PRO'\n",
    "    if pos[1] == 'CD':\n",
    "        p = 'NUMR'\n",
    "    pos_en_nltk.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3208955223880597"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pos_en_nltk, pos_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.help.upenn_tagset('WDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_en_spacy = []\n",
    "doc = nlp((' ').join(tokens_en))\n",
    "for sent in doc.sents:\n",
    "    for pos in sent:\n",
    "        pos = pos.pos_\n",
    "        if pos == 'PROPN':\n",
    "            pos = 'NOUN'\n",
    "        if (pos == 'PRON')or(pos == 'DET'):\n",
    "            pos = 'PRO'\n",
    "        if pos == 'AUX':\n",
    "            pos = 'VERB'\n",
    "        if pos == 'ADP':\n",
    "            pos = 'PREP'\n",
    "        if (pos == 'CCONJ')or(pos == 'SCONJ'):\n",
    "            pos = 'CONJ'\n",
    "        if pos == 'X':\n",
    "            pos = 'OTHER'\n",
    "        if pos == 'NUM':\n",
    "            pos = 'NUMR'\n",
    "        if pos == 'ADV':\n",
    "            pos = 'ADVB'\n",
    "        pos_en_spacy.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_en_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8208955223880597"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pos_en_spacy, pos_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner_2(text):\n",
    "    text = word_tokenize(text)\n",
    "    text_clean =[]\n",
    "    for word in text:\n",
    "        if (word not in '.,/\\|!?:;*—- ')and(word.startswith(\"'\") == False):\n",
    "            text_clean.append(word)\n",
    "        elif word.startswith(\"'\") == True:\n",
    "            text_clean[-1] = text_clean[-1] + word\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_en = sent_tokenize(text_en)\n",
    "for sent in enumerate(sents_en):\n",
    "    sents_en[sent[0]] = (' ').join(cleaner_2(sent[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 01:49:42,579 loading file C:\\Users\\VADIK\\.flair\\models\\en-pos-ontonotes-v0.5.pt\n",
      "2020-10-18 01:49:44,097 loading file C:\\Users\\VADIK\\.flair\\models\\en-pos-ontonotes-v0.5.pt\n",
      "2020-10-18 01:49:45,131 loading file C:\\Users\\VADIK\\.flair\\models\\en-pos-ontonotes-v0.5.pt\n",
      "2020-10-18 01:49:46,526 loading file C:\\Users\\VADIK\\.flair\\models\\en-pos-ontonotes-v0.5.pt\n",
      "2020-10-18 01:49:47,595 loading file C:\\Users\\VADIK\\.flair\\models\\en-pos-ontonotes-v0.5.pt\n"
     ]
    }
   ],
   "source": [
    "pos_en_flair = []\n",
    "for sent in sents_en:\n",
    "    sentence = Sentence(sent)\n",
    "    tagger = SequenceTagger.load('pos')\n",
    "    tagger.predict(sentence)\n",
    "    pos_s = re.findall(r'<.+?>', sentence.to_tagged_string())\n",
    "    pos_en_flair.extend(pos_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos in enumerate(pos_en_flair):\n",
    "    if pos[1] == '<DT>':\n",
    "        pos_en_flair[pos[0]] = 'DET'\n",
    "    if (pos[1] == '<NN>')or(pos[1] == '<NNP>')or(pos[1] == '<NNS>'):\n",
    "        pos_en_flair[pos[0]] = 'NOUN'\n",
    "    if (pos[1] == '<VBZ>')or(pos[1] == '<VB>')or(pos[1] == '<VBN>')or(pos[1] == '<VBP>'):\n",
    "        pos_en_flair[pos[0]] = 'VERB'\n",
    "    if (pos[1] == '<TO>')or(pos[1] == '<IN>'):\n",
    "        pos_en_flair[pos[0]] = 'PREP'\n",
    "    if pos[1] == '<JJ>':\n",
    "        pos_en_flair[pos[0]] = 'ADJ'\n",
    "    if pos[1] == '<CC>':\n",
    "        pos_en_flair[pos[0]] = 'CONJ'\n",
    "    if pos[1] == '<RB>':\n",
    "        pos_en_flair[pos[0]] = 'ADVB'\n",
    "    if (pos[1] == '<PRP>')or(pos[1] == '<WDT>'):\n",
    "        pos_en_flair[pos[0]] = 'PRO'\n",
    "    if pos[1] == '<CD>':\n",
    "        pos_en_flair[pos[0]] = 'NUMR'\n",
    "    if pos[1] == '<``>':\n",
    "        pos_en_flair[pos[0]] = 'OTHER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_en_flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9104477611940298"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pos_en_flair, pos_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
