{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hometask_3_Zaporoshchenko.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kaON4ga4mFR",
        "outputId": "bfeaefb2-d2b1-4f87-d92c-87da67c3a43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDLFTdUZ3awc"
      },
      "source": [
        "Анна Запорощенко. Запускается только в коллабе, на всякий случай вот ссылка: https://colab.research.google.com/drive/1heNI53qZhVIjnaDU7Tjm_uGtppAZd7T6?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDxKoYvd4n2Z",
        "outputId": "7305965f-c055-42e0-f4a0-ccfe7dcc2041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO82Vwnk4qNO",
        "outputId": "476166aa-376e-4513-a23a-4d0bdbc65960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!pip  install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.35.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (50.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.5.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (20.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H83raggJ4sBA",
        "outputId": "1554b47e-4772-42fe-a187-5a9860195ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "import nltk; nltk.download('stopwords')\n",
        "\n",
        "\n",
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa6x-0344t5_"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk2R7MIh4wE_"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBdFV1nq4yVj"
      },
      "source": [
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCX5ukxM41Fa"
      },
      "source": [
        "# Convert to list\n",
        "data = df.content.values.tolist()\n",
        "\n",
        "\n",
        "#YOURCODE\n",
        "\n",
        "# Remove Emails\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dwofJDs5hFo"
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJu5kr1B5z84"
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSIO7pNi53vA"
      },
      "source": [
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PpYf3iM5515"
      },
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS7XYahS58zQ"
      },
      "source": [
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygIxRt-s66xS"
      },
      "source": [
        "Mallet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCouq2rT6TVk",
        "outputId": "592456ae-13b0-4531-cd39-898ff7210ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "\n",
        "!pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.8.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xttt9puJ7Adu",
        "outputId": "44ed37ee-f621-418b-988d-031149c73dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import os       #importing os to set environment variable\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"11.0.8\" 2020-07-14\n",
            "OpenJDK Runtime Environment (build 11.0.8+10-post-Ubuntu-0ubuntu118.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.8+10-post-Ubuntu-0ubuntu118.04.1, mixed mode, sharing)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERwp-aUc7QI_",
        "outputId": "5cda5466-01ed-4984-89d2-83a8082499a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-19 08:23:59--  http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
            "Resolving mallet.cs.umass.edu (mallet.cs.umass.edu)... 128.119.246.70\n",
            "Connecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16184794 (15M) [application/zip]\n",
            "Saving to: ‘mallet-2.0.8.zip.1’\n",
            "\n",
            "mallet-2.0.8.zip.1  100%[===================>]  15.43M  15.3MB/s    in 1.0s    \n",
            "\n",
            "2020-10-19 08:24:01 (15.3 MB/s) - ‘mallet-2.0.8.zip.1’ saved [16184794/16184794]\n",
            "\n",
            "Archive:  mallet-2.0.8.zip\n",
            "replace mallet-2.0.8/bin/classifier2info? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T00cCvMi7XA8"
      },
      "source": [
        "os.environ['MALLET_HOME'] = '/content/mallet-2.0.8'\n",
        "mallet_path = '/content/mallet-2.0.8/bin/mallet' # you should NOT need to change this "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxm0vP9f-u86"
      },
      "source": [
        "Функция, которая подбирает оптимальное количество топиков\n",
        "Перебираем диапазон топиков от 18 до 28 (очень долго ждать, поэтому так), на каждом обучаем модель. Для каждой модели считаем метрику и выбираем то количество топиков, где coherence максимальная."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mIRp02J-0EE"
      },
      "source": [
        "def best_num_topics(mallet_path, corpus, data_lemmatized, id2word):\n",
        "  max_coherence = 0\n",
        "  best_number = 0\n",
        "  for i in range(18,28):\n",
        "    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=i, id2word=id2word)\n",
        "    coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "    coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
        "    print(i, ' ', coherence_ldamallet)\n",
        "    if coherence_ldamallet > max_coherence:\n",
        "      max_coherence = coherence_ldamallet\n",
        "      best_number = i\n",
        "  return best_number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-if1vPyDMKO",
        "outputId": "de7ff6f6-ce31-4860-f645-4383a546b24c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "best_num_topics(mallet_path, corpus, data_lemmatized, id2word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18   0.5311770708987563\n",
            "19   0.5398267468211734\n",
            "20   0.5354354454953549\n",
            "21   0.5444309282354114\n",
            "22   0.5283106777577377\n",
            "23   0.5237340440852971\n",
            "24   0.5457875766636415\n",
            "25   0.5328867196436825\n",
            "26   0.5294259230562613\n",
            "27   0.5319354273729553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLJrRd9HFM4G"
      },
      "source": [
        "Модель, обученная для лучшего количества топиков (24)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDzDlNhIWBnA"
      },
      "source": [
        "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=24, id2word=id2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhU98eeNW_-H"
      },
      "source": [
        "doc_lda = ldamallet[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUtSoLGhHHBL"
      },
      "source": [
        "У нас есть выдача модели по данным corpus. Там для каждого списка тем есть вероятность, с которой конкретный текст соответствует этой теме. Можно просто выбрать самое большое значение для каждого текста."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXr3lvQxYRnJ"
      },
      "source": [
        "def wide_topic_creation(doc_lda):\n",
        "  wide_topics = {}\n",
        "  for i in range(0,24): #генерируем словарь формата \"номер топика\":\"список\" (пока пустой)\n",
        "    wide_topics[i] = []\n",
        "  for text in enumerate(doc_lda): #для каждого текста задаем счетчик и перебором вытаскиваем самый вероятный топик\n",
        "    counter = 0\n",
        "    topic_number = 100\n",
        "    for topic in text[1]:\n",
        "      if topic[1] > counter:\n",
        "        counter = topic[1]\n",
        "        topic_number = topic[0]\n",
        "    wide_topics[topic_number] == wide_topics[topic_number].append(text[0]) #каждый раз пополняем список соответствующего топика в словаре\n",
        "  return wide_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OQRNxhqMH_m"
      },
      "source": [
        "wide_topics = wide_topic_creation(doc_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SeM9VDmMngc"
      },
      "source": [
        "Теперь считаем tf_idf и сразу будем выбирать по пять слов с самым высоким tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Gt7i-QMm0d"
      },
      "source": [
        "def td_idf(texts_id, texts): #функция берет id текстов для одного топика и все лемматизированные тексты\n",
        "  the_texts = []\n",
        "  for i in texts_id: #достаем только нужные\n",
        "    the_texts.append((' '.join(texts[i]))) #векторайзер работает с текстом, а не списком слов, поэтому объединяем\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectors = vectorizer.fit_transform(the_texts)\n",
        "  feature_names = vectorizer.get_feature_names() #вокабуляр\n",
        "  dense = vectors.todense()\n",
        "  denselist = dense.tolist() #список векторов, равных по длине вокабуляру, собственно, tf_idf\n",
        "  best_five = [] #инициализируем список для наборов по 5 слов\n",
        "  for text in denselist:\n",
        "    zipped = zip(text, feature_names) #склеиваем вектор каждого текста с вокабуляром\n",
        "    zipped = sorted(zipped, key=lambda x: x[0], reverse=True)[0:5] #сортируем по убыванию, оставляем 5 самых больших tf_idf\n",
        "    id_tdf, words = zip(*zipped) #разлепляем обратно\n",
        "    best_five.append(words)\n",
        "  return best_five"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCMD9awJd5GP"
      },
      "source": [
        "for topic_number, texts_id in wide_topics.items(): #будем пополнять уже созданный словарь wide_topics\n",
        "  best_five = td_idf(texts_id, texts)\n",
        "  id_best_five_dict = {} #инициализируем словарь для каждого топика\n",
        "  for i in enumerate(texts_id):\n",
        "    id_best_five_dict[data[i[1]]] = list(best_five[i[0]]) #внутри этого словаря находятся пары \"исходный текст\": \"5 слов с самым высоким ft_idf\"\n",
        "  wide_topics[topic_number] = id_best_five_dict # для каждого топика - по такому словарю"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvJzyDStgmZl"
      },
      "source": [
        "Теперь выведем таблицу"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-u6S3O_gX8X",
        "outputId": "b77597ed-2f1c-4251-e9c7-a953a9dfac5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df = pd.DataFrame(wide_topics)\n",
        "df.stack().reset_index()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level_0</th>\n",
              "      <th>level_1</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: (Bob Shaw) Subject: SUMMARY xon and X11R...</td>\n",
              "      <td>0</td>\n",
              "      <td>[semiconductor, symptom, folk, hi, user]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: (Benedikt Rosenau) Subject: Re: Gospel D...</td>\n",
              "      <td>0</td>\n",
              "      <td>[old, argument, obvious, evidence, know]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: (Bob Douglas) Subject: Re: Sphere from 4...</td>\n",
              "      <td>0</td>\n",
              "      <td>[plane, normal, lie, point, find]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: (Mark Zenier) Subject: Re: MC SBI mixer ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[circuit, theory, receiver, lot, book]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: (Ferdinand Oeinck) Subject: detecting do...</td>\n",
              "      <td>0</td>\n",
              "      <td>[detect, ferdinand, point, calculating_double,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11309</th>\n",
              "      <td>From: (Melinda . Hsu ) Subject: Re: The arroga...</td>\n",
              "      <td>23</td>\n",
              "      <td>[spiritual, choice, actual, faith, thing]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11310</th>\n",
              "      <td>From: Subject: Re: The arrogance of Christians...</td>\n",
              "      <td>23</td>\n",
              "      <td>[evidence, reason, faith, experience, explain]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11311</th>\n",
              "      <td>From: (Keith Allan Schneider) Subject: Re: Obj...</td>\n",
              "      <td>23</td>\n",
              "      <td>[omniscient, goal, specie, actualization, exte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11312</th>\n",
              "      <td>From: mathew Subject: Alt.Atheism FAQ: Atheist...</td>\n",
              "      <td>23</td>\n",
              "      <td>[book, atheism, humanism, address, history]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11313</th>\n",
              "      <td>From: (Mark Baker) Subject: Re: The arrogance ...</td>\n",
              "      <td>23</td>\n",
              "      <td>[evidence, reason, sense, physical, faith]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11314 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 level_0  ...                                                  0\n",
              "0      From: (Bob Shaw) Subject: SUMMARY xon and X11R...  ...           [semiconductor, symptom, folk, hi, user]\n",
              "1      From: (Benedikt Rosenau) Subject: Re: Gospel D...  ...           [old, argument, obvious, evidence, know]\n",
              "2      From: (Bob Douglas) Subject: Re: Sphere from 4...  ...                  [plane, normal, lie, point, find]\n",
              "3      From: (Mark Zenier) Subject: Re: MC SBI mixer ...  ...             [circuit, theory, receiver, lot, book]\n",
              "4      From: (Ferdinand Oeinck) Subject: detecting do...  ...  [detect, ferdinand, point, calculating_double,...\n",
              "...                                                  ...  ...                                                ...\n",
              "11309  From: (Melinda . Hsu ) Subject: Re: The arroga...  ...          [spiritual, choice, actual, faith, thing]\n",
              "11310  From: Subject: Re: The arrogance of Christians...  ...     [evidence, reason, faith, experience, explain]\n",
              "11311  From: (Keith Allan Schneider) Subject: Re: Obj...  ...  [omniscient, goal, specie, actualization, exte...\n",
              "11312  From: mathew Subject: Alt.Atheism FAQ: Atheist...  ...        [book, atheism, humanism, address, history]\n",
              "11313  From: (Mark Baker) Subject: Re: The arrogance ...  ...         [evidence, reason, sense, physical, faith]\n",
              "\n",
              "[11314 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8NqBjxd03Ey"
      },
      "source": [
        "Coherence score - это метрика качества, которая показывает, с какой вероятностью каждые два ключевых для топика слова могут встретиться вместе. Если в целом, то метрика показывает, насколько вообще близки/сочетаемы ключевые слова, соответственно, насколько похожи будут друг на друга тексты, которые распознаются в качестве подходящих для данного топика."
      ]
    }
  ]
}